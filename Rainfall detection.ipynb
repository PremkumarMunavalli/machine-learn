{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3207d5-cbad-4b7f-9c7f-0ff9c2abe370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# For handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# For XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# SHAP for explainability\n",
    "import shap\n",
    "\n",
    "df = pd.read_csv('/Users/premkumar/Downloads/creditcard.csv')\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "df.isnull().s8um()\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "df.fillna(df.mean(), inplace = True)\n",
    "\n",
    "Exploratory data analysis and feature engineering \n",
    "\n",
    "\n",
    "print(df.describe())\n",
    "# check class distribution\n",
    "fraud_count = df['class'].value_counts()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "# PCA for dimentionality reduction\n",
    "\n",
    "features = df.drop('Class', axis =1)\n",
    "labels = df['Class']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print('labels')\n",
    "\n",
    "fraut_count = df['Class'].value_counts()\n",
    "\n",
    "fraud_count = df['Class'].value_counts()\n",
    "\n",
    "print(fraud_count)\n",
    "\n",
    "print.column(class)\n",
    "\n",
    "print(df[\"class\"])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df[\"class\"])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print('class distribution')\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values in Each Column:\\n\", missing_values)\n",
    "\n",
    "sns.countplot(data=df, x='Class')\n",
    "plt.title('Class Distribution (Imbalanced)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data = df, x = 'Class')\n",
    "plt.title('Class distribution (Imbalanced)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data = df, x = \"Class\")\n",
    "plt.title(\"calss distribution (imbalanced)')\n",
    "          plt.show()\n",
    "\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "X = df.drop(\"class', axis = 1)\n",
    "            y = df['class']\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column \", missing_values)\n",
    "\n",
    "df.isnull(), it checks in every cell in the dataframe df to see if it contains a missing value,\n",
    "\n",
    ".sum, adds up the number of true values in each column \n",
    "\n",
    "\n",
    "missing_values \n",
    "\n",
    "print(\"missing avalues in each column\", missing_values)\n",
    "\n",
    "why is handling missing data important ?\n",
    "df.isnull())\n",
    "\n",
    "# Scale data before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_data = df.drop('Class', axis=1)\n",
    "y_data = df['Class']\n",
    "\n",
    "# Scale data before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 1) Define your features and labels:\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# 2) Initialize the scaler and scale the data:\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3) (Optional) PCA or modeling steps\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "X = df.drop(\"Class\", axis = 1)\n",
    "y = d\n",
    "\n",
    "df.drop('Class', axis=1, inplace=True)\n",
    "\n",
    "X = df.drop['class = axis = 1')\n",
    "y = df.columns\n",
    "\n",
    "what does it mean, it calucualtes each feautes \n",
    "\n",
    "what is pca, pca is a dimenionality reduction technique that ransforms a set of \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=10, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"\\nPCA - Explained Variance Ratio (first 10 components):\\n{pca.explained_variance_ratio_}\")\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "Linear Transformation: PCA finds linear combinations of your original feature that are uncorerealte\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=32,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "df_clf = DecisionTreeClassifier(random_state = 32)\n",
    "df_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "# Check class distribution\n",
    "fraud_count = df['Class'].value_counts()\n",
    "print(fraud_count)\n",
    "\n",
    "sns.countplot(data=df, x='Class')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# what is desampling \n",
    "\n",
    "\n",
    "from sklear.model_selection import train_test_split, GridSearchCV, corss_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report,  accuracy_score\n",
    "\n",
    "imort pickle\n",
    "\n",
    "# random choose , gridSearchCV, takes from random, GridSearchCV , ramfom is taking lesser time\n",
    "\n",
    "# confusion matrix - true positive and true negative score, pickle to save the model as file\n",
    "\n",
    "# data collection and processing \n",
    "\n",
    "\n",
    "data = pd.read_csv('/Users/premkumar/Downloads/rainfall.csv')\n",
    "\n",
    "\n",
    "data.tail()\n",
    "\n",
    "data.head()\n",
    "\n",
    "data['day']unique()\n",
    "\n",
    "data['day'].unique()\n",
    "\n",
    "\n",
    "# we have different columns , we have days of a month, max tempe, min, temp, dew point,\n",
    "wind direction, we dont know it yet,\n",
    "\n",
    "so now we can see that data.tail()\n",
    "\n",
    "if we want to understand the unique calues \n",
    "\n",
    "data[\"day\"].unique()\n",
    "\n",
    "in the next step, print the data info()\n",
    "\n",
    "data.info()\n",
    "\n",
    "we have 366 intries, \n",
    "we have 366 non-values that is we have wind direction or \n",
    "\n",
    "data.info()\n",
    "\n",
    "#remove extra spaces in all columns\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "data.columns()\n",
    "\n",
    "data.columns\n",
    "\n",
    "#lets just add some importance to it\n",
    "\n",
    "data = data.drop(columns = [\"day\"])\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "# we have\n",
    "print(data.isnull().sum())\n",
    "\n",
    "we have too many outlayers, then we can handle the data \n",
    "\n",
    "if we dont have any outlayers\n",
    "\n",
    "#lets also look at the outlayer data\n",
    "\n",
    "whether you are doing to replace the replaced values \n",
    "\n",
    "#so we have hum, temp, \n",
    "\n",
    "data[\"winddirection\"].unique()\n",
    "\n",
    "data[\"winddirection\"].unique()\n",
    "data.columns\n",
    "\n",
    "data.columns\n",
    "\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data[\"winddirection\"] = data[\"winddirection\"].fillna(data[\"winddirection\"].mode()\n",
    "                                                     data[\"windspeed\"] = data[\"winddirection\"].fillna(data[\"windspeed\"].median())\n",
    "\n",
    "# we are working with data of winddrection, fill the na values , those are\n",
    "basically with the mode value or the most repeated value in the column \n",
    "\n",
    "# we are replacing the values ,\n",
    "apart from this we have numericals , lets replace yes with 1 and no with 0\n",
    "\n",
    "# data[\"rainfall\"].unique()\n",
    "\n",
    "\n",
    "\n",
    "data[\"rainfall\"].unique()\n",
    "\n",
    "\n",
    "data[\"rainfall\n",
    "\n",
    "data[\"rainfall\n",
    "\n",
    "data[\"rainfall\"].unique()\n",
    "\n",
    " #converting the yes and no to 1 and 0 respectively\n",
    "\n",
    "data[\"rainfall\"] = data[\"rainfall\n",
    "\n",
    "data[\"rainfall\n",
    "\n",
    "data[\"rainfall\"] = data[\"rainfall\"}.map(\"yes\":1, \"no\":0)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.head()\n",
    "\n",
    "# we are doing some data analysis\n",
    "#exploratory data analysis (EDA)\n",
    "\n",
    "data.shape\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "we have sns.set(style = \"whitegrid\")\n",
    "\n",
    "we have with white background \n",
    "\n",
    "#etting the plot style for all the plotes \n",
    "sns.set(style ='whitegrid')\n",
    "\n",
    "data.describe()\n",
    "\n",
    "# here we have the mean of each of these columns \n",
    "\n",
    "data.describe()\n",
    "\n",
    "data.desribe()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "# when we have mean and median value \n",
    "\n",
    "data.describe()\n",
    "\n",
    "sns.set(style = \"whitegrid\")\n",
    "\n",
    "# we have the count , we have the mean of each of these points , we have std diviation we have percentage value , that is \n",
    "\n",
    "\n",
    "\n",
    "# we can leave the wind direction, we have wind speed that is 101 .28\n",
    "\n",
    " from sklearn.model_selection import train_test_split, GridSearchCV, \n",
    "\n",
    "also we are getting on the testsplit \n",
    "\n",
    "\n",
    "\n",
    "sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data.tail()\n",
    "\n",
    "data[\"day\"].unique()\n",
    "\n",
    "print(\"data info\")\n",
    "data.info()\n",
    "\n",
    "we have 366 entries\n",
    "\n",
    "data = data.drop(columns = [\"day\"])\n",
    "\n",
    "data = data.drop(columns = [\"day\"])\n",
    "\n",
    " # checking the numerber of missing values \n",
    "data.isnull().sum()\n",
    "data[\"winddirection\"].unique()\n",
    "\n",
    "data.columns\n",
    "\n",
    "df.columns[df.isna().sum() > 0]\n",
    "\n",
    "\n",
    "df = df.fillna(value)\n",
    "\n",
    "\n",
    "pint(data.isnull().sum())\n",
    "\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data[\"wind_condition\"].unique()\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "\n",
    "data[\"wind_condition\"].unique()\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.describe()\n",
    "\n",
    "df = df.fillna(0)  # Replace NaN with 0\n",
    "\n",
    "\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(df.isnull().sum())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "data[\"winddirection\"] = data[\"winddirection\". mode()[0])\n",
    "\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data[\"winddirection\"].unique()\n",
    "\n",
    "data[\"wind_condition\"].unique()\n",
    "\n",
    "print(data.isnull().sum())\n",
    "\n",
    "\n",
    "# Fill missing values for the specified columns with mode\n",
    "columns_to_fill_mode = [\"rainfall\", \"sunshine\", \"winddirection\", \"wind_speed\"]\n",
    "\n",
    "for column in columns_to_fill_mode:\n",
    "    if column in data.columns:\n",
    "        data[column] = data[column].fillna(data[column].mode()[0])\n",
    "    else:\n",
    "        print(f\"Column '{column}' does not exist in the dataset.\")\n",
    "\n",
    "\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Fill missing values with mode or mean/median as appropriate\n",
    "columns_to_fill_mode = [\"weather_condition\"]\n",
    "columns_to_fill_mean = [\"temperature\", \"humidity\"]\n",
    "\n",
    "# Fill with mode for categorical columns\n",
    "for column in columns_to_fill_mode:\n",
    "    if column in data.columns:\n",
    "        data[column] = data[column].fillna(data[column].mode()[0])\n",
    "\n",
    "# Fill with mean for numerical columns\n",
    "for column in columns_to_fill_mean:\n",
    "    if column in data.columns:\n",
    "        data[column] = data[column].fillna(data[column].mean())\n",
    "\n",
    "# Verify missing values are handled\n",
    "missing_values = data.isna().sum()\n",
    "print(missing_values)\n",
    "\n",
    "\n",
    "# Check unique values in the column\n",
    "print(data[\"rainfall\"].unique())\n",
    "\n",
    "# Convert 'yes' and 'no' to 1 and 0\n",
    "data[\"rainfall\"] = data[\"rainfall\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "sns.set(style = \"whitegrid\")\n",
    "\n",
    " plt.figure(figsize = (15,10))\n",
    "\n",
    "for i, column in enumerate (['pressure', 'maxtemp', 'temperature', 'mintemp', 'dewpoint', 'humidity', 'cloud', 'sunshine', 'windspeed'],1):plt.subplot(3,3,i)\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "for i, column in enumerate (['pressure', 'maxtemp', 'temperature', 'mintemp', \n",
    "                             'dewpoint', 'humidity', 'cloud', 'sunshine', 'windspeed'],1);\n",
    "plt.subplot(3,3,i)\n",
    "\n",
    "we have total overl 9 plits, and 3,3 is 3 rows and 3 columns \n",
    "\n",
    "for i, column in enumerate(['pressure', 'maxtemp', 'temprature', 'mintemp', 'dewpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# List of columns to plot\n",
    "columns = ['pressure', 'maxtemp', 'temperature', 'mintemp', 'dewpoint', 'humidity', 'cloud']\n",
    "\n",
    "# Loop through each column and create subplots\n",
    "for i, column in enumerate(columns, start=1):\n",
    "    plt.subplot(3, 3, i)  # Create a grid of 3x3 subplots\n",
    "    sns.histplot(data[column], kde=True)  # Plot histogram with KDE\n",
    "    plt.title(f\"Distribution of {column}\")  # Add title\n",
    "    plt.tight_layout()  # Adjust subplot layout to fit\n",
    "plt.show()  # Display the plots\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# List of columns to plot\n",
    "columns = ['pressure', 'maxtemp', 'temperature', 'mintemp', 'dewpoint', 'humidity', 'cloud']\n",
    "\n",
    "# Check which columns exist in the dataset\n",
    "existing_columns = [col for col in columns if col in data.columns]\n",
    "\n",
    "if not existing_columns:\n",
    "    print(\"None of the specified columns exist in the dataset.\")\n",
    "else:\n",
    "    # Loop through existing columns and create subplots\n",
    "    for i, column in enumerate(existing_columns, start=1):\n",
    "        plt.subplot(3, 3, i)  # Create a grid of 3x3 subplots\n",
    "        sns.histplot(data[column], kde=True)  # Plot histogram with KDE\n",
    "        plt.title(f\"Distribution of {column}\")  # Add title\n",
    "        plt.tight_layout()  # Adjust subplot layout to fit\n",
    "    plt.show()  # Display the plots\n",
    "\n",
    "\n",
    "sns.histplot(data, kde=True, bins=30, color='blue')  # kde=True adds a density curve\n",
    "plt.title('Histogram with KDE')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data, kde = True, bins = 30, color = 'blue')\n",
    "plt.title('histogram with kde')\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()\n",
    "\n",
    "#we are skipping the standardization , we we are working on tree based omosdel\n",
    "\n",
    "plt. figure(figsize=(6, 4))\n",
    "sns.countplot(x=\"rainfall\", data=data)\n",
    "plt.title(\"Distribution of Rainfall\")\n",
    "plt. show() \n",
    "\n",
    "print(data[\"rainfall\"].unique())  # Check unique values\n",
    "print(data[\"rainfall\"].dtype)    # Check the data type\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))  # Adjust figure size\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")  # Add `fmt` to format the annotations\n",
    "plt.title(\"Correlation Heatmap\")  # Set the title\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_data = data.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Check if there are numerical columns\n",
    "if numerical_data.empty:\n",
    "    print(\"No numerical columns in the dataset for correlation.\")\n",
    "else:\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))  # Adjust figure size\n",
    "    sns.heatmap(numerical_data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")  # Add `fmt` to format the annotations\n",
    "    plt.title(\"Correlation Heatmap\")  # Set the title\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(15, len(numerical_columns) * 4))\n",
    "\n",
    "# Loop through numerical columns and create box plots\n",
    "for i, column in enumerate(numerical_columns, start=1):\n",
    "    plt.subplot(len(numerical_columns), 1, i)\n",
    "    sns.boxplot(data[column], color='skyblue')\n",
    "    plt.title(f'Box Plot for {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "  data = data.drop(columns = ['maxtemp', 'temperature', 'mintemp'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Reloading the dataset after reset\n",
    "file_path = '/mnt/data/rainfall (1).csv'\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"Rainfall Dataset\", dataframe=data)\n",
    "    data.head()  # Display the first few rows of the dataset\n",
    "except Exception as e:\n",
    "    str(e)\n",
    "\n",
    "\n",
    "# Check unique values in the 'rainfall' column\n",
    "print(data[\"rainfall\"].unique())\n",
    "\n",
    "\n",
    "# Convert 'rainfall' column from 'yes/no' to 1/0\n",
    "data[\"rainfall\"] = data[\"rainfall\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "\n",
    "# Convert 'rainfall' column from 'yes/no' to 1/0\n",
    "data[\"rainfall\"] = data[\"rainfall\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "# Display the updated 'rainfall' column to verify changes\n",
    "print(data[\"rainfall\"].head())\n",
    "\n",
    "\n",
    "# Strip whitespace and convert to lowercase\n",
    "data[\"rainfall\"] = data[\"rainfall\"].str.strip().str.lower()\n",
    "\n",
    "# Map values again\n",
    "data[\"rainfall\"] = data[\"rainfall\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "# Fill NaN with a default value (optional)\n",
    "data[\"rainfall\"] = data[\"rainfall\"].fillna(0)  # Replace NaN with 0\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "data.head()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Replace NaN values in 'rainfall' with random choices of 'yes' or 'no'\n",
    "data['rainfall'] = data['rainfall'].fillna(np.random.choice(['yes', 'no'], size=data['rainfall'].isna().sum()))\n",
    "\n",
    "# Verify the changes\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Replace NaN values in 'rainfall' with random choices of 'yes' or 'no'\n",
    "data.loc[data['rainfall'].isna(), 'rainfall'] = np.random.choice(['yes', 'no'], size=data['rainfall'].isna().sum())\n",
    "\n",
    "# Verify the changes\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "import numpy\n",
    "as npl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt import seaborn as sns from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn metrics import classification_report, confusion_matrix, accuracy_score import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Replace NaN values in 'rainfall' with random choices of 'yes' or 'no'\n",
    "data.loc[data['rainfall'].isna(), 'rainfall'] = np.random.choice(['yes', 'no'], size=data['rainfall'].isna().sum())\n",
    "\n",
    "# Verify the changes\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "data collection and precessing\n",
    "\n",
    "\n",
    "df[\n",
    "\n",
    "data [\"day\"]. unique ( )\n",
    "array?\n",
    "\n",
    "print(\"data info:\")\n",
    "data.info()\n",
    "\n",
    "data[\"rainfall\"].unique()\n",
    "data[\"rainfall\"].unique()\n",
    "\n",
    "data[\"rainfall\"] = data[\"rainfall\"].map({\"yes\":1, \"no\":0})\n",
    "\n",
    "data[\"rainfall\"] = mapping { yes : 1, no : 0\n",
    "                            \n",
    "\n",
    "df.head()\n",
    "\n",
    "data.head()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Adjust figure size\n",
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "# Create a count plot\n",
    "sns.countplot(data=data, x=\"pressure\")  # Ensure \"pressure\" is a valid column in the dataset\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Count Plot for Pressure\")\n",
    "plt.xlabel(\"Pressure\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#sns.countplot(data = data, x = \"pressure\")\n",
    "here we are ensuring the pressure  \n",
    "\n",
    "#add title \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure \"pressure\" is a valid column in the dataset\n",
    "if \"pressure\" in data.columns:\n",
    "    # Adjust figure size\n",
    "    plt.figure(figsize=(9, 3))\n",
    "    \n",
    "    # Create a histogram if \"pressure\" is numerical\n",
    "    if pd.api.types.is_numeric_dtype(data[\"pressure\"]):\n",
    "        sns.histplot(data=data, x=\"pressure\", bins=10, kde=False)\n",
    "        plt.title(\"Histogram for Pressure\")\n",
    "    else:\n",
    "        # Create a count plot if \"pressure\" is categorical\n",
    "        sns.countplot(data=data, x=\"pressure\")\n",
    "        plt.title(\"Count Plot for Pressure\")\n",
    "    \n",
    "    # \n",
    "\n",
    "\n",
    "data.describe()\n",
    "\n",
    "Lets have downsampling\n",
    "\n",
    "# which is majority and minority class\n",
    "\n",
    "df[\"rainfall\"].value_counts())\n",
    "\n",
    "data[\"rainfall\"].value_counts()\n",
    "\n",
    "\n",
    "df[\"rainfall\"].value_counts()\n",
    "\n",
    "data[\"rainfall\"].value_counts()\n",
    "\n",
    "\n",
    "df_majority = data[data[\"rainfall\"] -- 1]\n",
    "df_minority = data[data[\"rainfall\"] -- 0]\n",
    "\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = data[data[\"rainfall\"] == 1]\n",
    "df_minority = data[data[\"rainfall\"] == 0]\n",
    "\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Downsample the majority class\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority,\n",
    "    replace=False,              # Do not replace samples (sampling without replacement)\n",
    "    n_samples=len(df_minority), # Match the number of samples in the minority class\n",
    "    random_state=42             # Ensure reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "# Combine the downsampled majority class and the minority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Shuffle the dataset (optional, for randomizing rows)\n",
    "df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the class distribution\n",
    "print(df_downsampled[\"rainfall\"].value_counts())\n",
    "\n",
    "\n",
    "df_majority_downsampled.shape\n",
    "\n",
    "\n",
    "df_minority.shape\n",
    "\n",
    "df_downsampled.head()\n",
    "\n",
    "#we are shuffling the final dataframe\n",
    "df_downsampled = df_downsampled.sample\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "\n",
    "df_downsampled.head()\n",
    "\n",
    "df_downsampled[\"rainfall\"].value_counts()\n",
    "\n",
    "df_downsampled[\"rainfall\"].value_counts()\n",
    "\n",
    "#splitting the data into training and test data\n",
    "\n",
    "# print(data[\"rainfall\"].value_counts())\n",
    "\n",
    "\n",
    "# we are downsampling the value to \n",
    "from 249 to 117\n",
    "\n",
    "df_majority  = data[data[\"rainfall\"] == 1]\n",
    "df_minority = data[data[\"rainfall\"] ==0 ]\n",
    "\n",
    "print(df_majority.shape)\n",
    "print(df_minority.shape)\n",
    "\n",
    " df_downsampled = df.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_downsampled.shape\n",
    "\n",
    "df_downsampled.shape\n",
    "\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_downsampled.shape\n",
    "\n",
    "#shuffle the final dataframe\n",
    "\n",
    "df_downsample = df_downsampled.sample(frac=1, random_state =42).reset_index(drop=True)\n",
    "\n",
    "df_downsampled.value_counts()\n",
    "\n",
    "df_downsampled[\"rainfall\"].value_counts()\n",
    "\n",
    "#we have performed the downsampling successfully\n",
    "\n",
    "#we are splitting the data into train and test ddata\n",
    "# we wil split the data into features and target as X and y \n",
    "\n",
    "X = df_downsampled.drop(columns = [\"rainfall\"])\n",
    "y = df_downsampled[\"rainfall\"]\n",
    "\n",
    "print(X)\n",
    "\n",
    "#2. Why Are Features Considered Independent?\n",
    "In the context of supervised learning:\n",
    "\n",
    "Features are independent of each other in terms of modeling:\n",
    "The model assumes that the relationship between the features (X) and the target (y) is what matters.\n",
    "The features provide the inputs, and the model determines how they combine to predict the target.\n",
    "However, in reality:\n",
    "Features can be correlated (e.g., humidity and temperature may have some relationship).\n",
    "Strong correlations between features can lead to multicollinearity, which may affect model performance.\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Base model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100, 200],               # Number of trees\n",
    "    \"max_features\": [\"sqrt\"],                    # Number of features to consider at each split\n",
    "    \"max_depth\": [None, 10, 20, 30],             # Maximum depth of the tree\n",
    "    \"min_samples_split\": [2, 5, 10],             # Minimum samples required to split an internal node\n",
    "    \"min_samples_leaf\": [1, 2, 4]                # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring=\"accuracy\",      # Metric to optimize\n",
    "    verbose=2,               # Display progress\n",
    "    n_jobs=-1                # Use all processors for parallelization\n",
    ")\n",
    "\n",
    "# Fit grid search on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "test_accuracy = best_rf_model.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=best_rf_model,  # Best Random Forest model from GridSearchCV\n",
    "    X=X_train,                # Features (training data)\n",
    "    y=y_train,                # Target variable (training data)\n",
    "    cv=5,                     # Number of cross-validation folds\n",
    "    scoring=\"accuracy\"        # Metric to evaluate (accuracy in this case)\n",
    ")\n",
    "\n",
    "# Print the cross-validation scores and their mean\n",
    "print(\"Cross-Validation Scores for Each Fold:\", cv_scores)\n",
    "print(\"Mean Cross-Validation Score:\", cv_scores.mean())\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\" : [50, 100, 200],\n",
    "    \"max_features\" : [\"sqrt\", \"log2\"],\n",
    "    \"max_depth\" : [none, 10,20,30]\n",
    "    \"min_samples_split\" : [2,5,10],\n",
    "\"min_samples_leaf\" : [1,2,4]\n",
    "\n",
    "# lets run this ,\n",
    "we are doing the task of grid search ,\n",
    "# model training , lets this as bold, \n",
    "param_grid_rf = {\n",
    "    n_estimators \n",
    "\n",
    "    w\n",
    "\n",
    "# model evalueation :\n",
    "we can go a normal fit \n",
    "\n",
    "cv_scores - corss_val_score(best_rf_model, X_train, y_train, cv-5)\n",
    "print(\"cross-validation scores\", cv_score)\n",
    "print(\"corss mean corss validation scores\", np.mean(cv_scores.mean())\n",
    "\n",
    "cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", np.mean(cv\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
    "\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "df.head()\n",
    "\n",
    "data.head()\n",
    "\n",
    "#checking the number of missing values \n",
    "for this we can say\n",
    "\n",
    "\n",
    "# handle the missing values \n",
    "data[\"winddirectio n\".unique()]\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb74acb-213d-4071-8837-89057a1031ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218dd0c8-eb4f-4796-8104-736b330784b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a203813-f903-45a1-9597-c35dc02a4a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2614e2-be8a-4a60-a70c-485cbaa90681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd5efe-37d4-4bca-9938-fc97e7758599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa02bb-471a-4f30-aba4-5f2c52ee7bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8370e1b-a0c2-4411-bffc-92d838f3b563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e593b96-0938-4022-be31-d07b95b6f023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
